# 第3章 連立1次方程式

## 3.1 直接法と反復法

### 3.1.1 直接法・反復法

実数の係数をもつ $n$ 元連立1次方程式

$$
\begin{array}{ll}
    a_{11} x_1 + a_{12} x_2 + a_{13} x_3 + \cdots + a_{1n} x_n & = b_1 \quad (1) \\
    a_{21} x_1 + a_{22} x_2 + a_{23} x_3 + \cdots + a_{2n} x_n & = b_2 \quad (2) \\
    a_{31} x_1 + a_{32} x_2 + a_{33} x_3 + \cdots + a_{3n} x_n & = b_3 \quad (3) \\
    \cdots \, \cdots \\
    a_{i1} x_1 + a_{i2} x_2 + a_{i3} x_3 + \cdots + a_{in} x_n & = b_i \quad (\,i\,) \\
    \cdots \, \cdots \\
    a_{n1} x_1 + a_{n2} x_2 + a_{n3} x_3 + \cdots + a_{nn} x_n & = b_n \quad (n) \\
\end{array} \tag{3.1}
$$

の数値解法は、大きく分けて直接法と反復法の2つがある。
直接法は、手順を1回実行すれば解が求められる。
反復法は、必要な精度の近似値が得られるまで、手順を反復する。
直接法は、解を求めるまでの演算回数は少なくてよいが、とくに大型行列の連立1次方程式の場合、高い精度の近似解が得にくい。
それは、直接法では解を求める途中で係数を書き換えてしまうが、書き換えの過程で誤差が発生することによる。
反復法では最後まで係数はもとのままである。
したがって、小さな連立1次方程式は直接法によることが多く、大型の連立1次方程式は反復法によることが多い。

本章で取り上げる連立1次方程式の数値解法は、次の通りである。

$$
\begin{array}{clll}
1 & \text{ガウスの消去法} & \text{直接法} \\
2 & \text{LU分解法} & \text{直接法} \\
3 & \text{コレスキー分解法} & \text{直接法} & \text{対称行列} \\
4 & \text{ヤコビ法} & \text{反復法} \\
5 & \text{ガウス・ザイデル法} & \text{反復法} \\
6 & \text{SOR法} & \text{反復法} \\
7 & \text{共役勾配法} & \text{ベクトル反復法} & \text{対称行列 (非対称行列)} \\
\end{array}
$$

ガウスの消去法はもっとも基本的な直接法である。
$\text{LU}$ 分解法は、ガウスの消去法をやや一般化して使いやすくしたものである。
コレスキー分解法は、 $\text{LU}$ 分解法を対称行列用に改造して半分の演算量で解を求めることのできる方法である。

ヤコビ法はもっとも基本的な反復法である。
ガウス・ザイデル法はヤコビ法を簡単にした結果、収束が早くなり、かつ記憶容量も少なくなっている。
SOR 法はガウス・ザイデル法の収束を加速した方法である。

共役勾配法は、対称な大型行列用の方法であり、近年にわかに使用されるようになっている。
とくに収束を加速した共役勾配法 (ICCG 法など) は、大型対称行列に対する主流となっている。
そのほか、本章の問題中には、非対称行列用の共役残差法 (CR 法)、双共役勾配法 (BCG 法) および自乗共役勾配法 (CGS 法) についてふれる。
これらの方法はベクトル反復法と呼ばれている。

以上のように、ここで説明する7つの方法は、直接法と反復法の2つの系統に分類される。
解くべき連立1次方程式の性質によって、これらの方法を使いわける必要がある。

## 3.2 ガウスの消去法

### 3.2.1 ガウスの消去法

連立1次方程式 $(3.1)$ の数値解法の中でもっとも基本的な解法は、ここで述べるガウスの消去法である。
筆算による連立1次方程式の解法でもっともよく用いられる解法は、消去法によって未知数を消去し、最後に残った未知数を求め、以下代入法によって順次残りの未知数を求める方法であろう。
ガウスの消去法は、筆算における消去法と代入法を順序立てて手順にまとめあげたものでもある。

### 3.2.2 前進消去

$n$元連立1次方程式 $(3.1)$ において、まず未知数 $x_1$ を消去する。
それには、第1式 $(1)$ を $x_1$ の係数 $a_{11}$ で割って、

$$
x_1 + a_{12}^{(1)} x_2 + a_{13}^{(1)} x_3 + \cdots + a_{1n}^{(1)} x_n = b_{1}^{(1)} \quad (1^{\prime})
$$

とし 、次いで第2式 $(2)$ から $(1^{\prime})$ の $a_{21}$ 倍を引くと $x_1$ が消去される：

$$
a_{22}^{(1)} x_2 + a_{23}^{(1)} x_3 + \cdots + a_{2n}^{(1)} x_n = b_{2}^{(1)} \qquad (2^{\prime})
$$

一般に第 i 式 $(i)$ から $(1^{\prime})$ の $a_{i1}$ 倍を引くと、 $x_1$ が消去され

$$
a_{i2}^{(1)} x_2 + a_{i3}^{(1)} x_3 + \cdots + a_{in}^{(1)} x_n = b_{i }^{(1)} \qquad (i^{\prime})
$$

が得られる。
$i = 2, 3, \cdots, n$ の順に繰り返すと、第2式, $\cdots$ 第 $n$ 式から $x_1$ が消去される。
このとき、新しい係数 $a_{ij}^{(1)}$ 、 $b_i^{(1)}$ は、

$$
\begin{array}{lll}
\text{第1行} & a_{11}^{(1)} = 1 \\
           & a_{1j}^{(1)} = a_{1j} / a_{11} & j > 1 \\
           & b_1^{(1)} = b_1 / a_{11} \\

\text{第2行以下} & a_{i1}^{(1)} = 0 & i > 1 \\
           & a_{ij}^{(1)} = a_{ij} - a_{i1} a_{1j}^{(1)} & i > 1, \quad j > 1 \\
           & b_i^{(1)} = b_i - a_{i1} b_1^{(1)} & i > 1 \\
\end{array}
$$

である。
こうして、第2式以降から $x_1$ が消去された。

次ぎに、第3式以降から $x_2$ を消去するには、第2式 $(2^{\prime})$ を $a_{22}^{(1)}$ で割って、

$$
x_2 + a_{23}^{(2)} x_3 + \cdots + a_{2n}^{(2)} x_n = b_2^{(2)} \qquad (2^{\prime\prime})
$$

とし、第 $i$ 式 $(i^{\prime})$ (ただし $i > 2$ ) 以降から $(2^{\prime\prime})$ の $a_{i2}^{(1)}$ 倍を引けばよい：

$$
a_{i3}^{(2)} x_3 + a_{i4}^{(2)} x_4 + \cdots + a_{in}^{(2)} x_n = b_{i}^{(2)} \qquad (i^{\prime\prime})
$$

ただし 、

$$
\begin{array}{lll}
\text{第2行} & a_{22}^{(2)} = 1 \\
 & a_{2j}^{(2)} = a_{2j}^{(1)} /a_{22}^{(1)} & j > 2 \\
 & b_{2}^{(2)} = b_{2}^{(1)} / a_{22}^{(1)} \\
\text{第3行以下} & a_{i2}^{(2)} = 0 & i > 2 \\
 & a_{ij}^{(2)} = a_{ij}^{(1)} - a_{i2}^{(1)} a_{2j}^{(2)} & i > 2, \quad j > 2 \\
 & b_{i}^{(2)} = b_{i}^{(1)}- a_{i2}^{(1)} b_{2}^{(2)} & i > 2 \\
\end{array}
$$

いま第 $k$ 式以降の式の $x_1$ からまで $x_{k-1}$ までがすでに消去されているとき、第 $k + 1$ 式以降から $x_k$ を消去することを考える。
それには、第 $k$ 式を $a_{kk}^{(k-1)}$ で割り、第 $k + 1$ 式以降の第 $i$ 式 $(i > k)$ から、第 $k$ 式 $(a_{kk}^{(k)} = 1)$ の $a_{kj}^{(k-1)}$ 倍を引けばよい。
すなわち、 $k = 1, 2, \cdots, n$ の順に

$$
\begin{array}{ll}
\text{ガウスの消去法の前進消去} \\
\quad a_{kk}^{(k)} = 1 \\
\quad a_{kj}^{(k)} = a_{kj}^{(k-1)} / a_{kk}^{(k-1)} & j > k \\
\quad b_{k}^{(k)} = b_{k}^{(k-1)} / a_{kk}^{(k-1)} \\
\quad a_{ik}^{(k)} = 0 & i > k \\
\quad a_{ij}^{(k)} = a_{ij}^{(k-1)} - a_{ik}^{(k-1)} a_{kj}^{(k)} & i > k, j > k \\
\quad b_{i}^{(k)} = b_{i}^{(k-1)} - a_{ik}^{(k-1)} b_{k}^{(k)} & i > k \\
\end{array} \tag{3.2}
$$

と係数と定数項を変化させると、対角要素はすべて $1$ になり、対角要素の下の要素はすべて $0$ になり、係数行列は上三角行列になる。
<details><summary></summary><div>

$(3.2)$ を PAD で表したものが、図 3.1 の 前進消去の部分である。
この PAD では、 $a_{ij}^{(k)}$ はすべての $k$ に対して同じ配列要素 `a(i,j)` に次々と上書きしている。
それは、 $a_{ij}^{(k)}$ が求められた後では $a_{ij}^{(k-1)}$ 以前の配列要素 `a(i,j)` の値を再び参照することはないからである。
また、 PAD では、対角要素 $1$ と下三角行列の $0$ はわかりきっているので、 $1$ にしたり $0$ にしたりする演算は行なっていない<sup>[$1)$](#note1)</sup> 。

> <small id="note1">$^{1)}$ $e_{n-1}(z)$ は $n - 1$ 次のラグランジュ補間多項式による $z^s$ の近似の誤差である。 $(5.53)$ 、 $(5.57)$ 参照。</small>
</div></details>

$(3.2)$ の手順が完了したときには、連立1次方程式は、次の形に変形されている。

$$
\begin{aligned}
x_1 + a_{12}^{(1)} x_2 + a_{13}^{(1)} x_3 + \cdots + a_{1n-1}^{(1)} x_{n-1} + a_{1n}^{(1)} x_n & = b_1^{(1)} \\
x_2 + a_{23}^{(2)} x_3 + \cdots + a_{2n-1}^{(2)} x_{n-1} + a_{2n}^{(2)} x_n & = b_2^{(2)} \\
x_3 + \cdots + a_{3n-1}^{(3)} x_{n-1} + a_{3n}^{(3)} x_n & = b_3^{(3)} \\
\cdots \, \cdots \\
x_{n-1} + a_{n-1n}^{(n-1)} x_n & = b_{n-1}^{(n-1)} \\
x_n & = b_n^{(n)}
\end{aligned} \tag{3.2’}
$$

<details><summary></summary><div>

なお、 PAD では同じ 係数 $A$ を持つ $m$ 個の問題を一挙に解くことができるようにしてある。
そのために、定数項は係数の行列の $第 n + 1 列, n + 2 列, \cdots n + m
列$ に記憶させてある。
すなわち、第 $i$ 行は

$$
\begin{array}{}
a_{i\ n+1} = b_{i\ 1}, & a_{i\ n+2} = b_{i\ 2}, & \cdots & a_{i\ n+m} = b_{i\ m}
\end{array}
$$

である。
こうしておくと PAD 、したがって手順が簡単になる。
ピボット選択については後で述べる。
</div></details>

### 3.2.3 後退代入

前進消去の結果 $x_n$ は解けている。
$x_{n-1}$ は最後から2番目の式を用いて、

$$
x_{n-1} = b_{n-1}^{(n-1)} - a_{n-1n}^{(n-1)} x_n
$$

以下同じようにして $k = n - 1, n - 2, \cdots , 2, 1$ と逆順に代入を繰り返して行けば、 $x_k$ が

$$
\begin{array}{r}
\text{ガウスの消去法の後退代入} \\
x_k = b_k^{(k)} - \sum\limits_{j=k+1}^n a_{kj}^{(k)} x_j
\end{array} \tag{3.3}
$$

と求められる。
これを後退代入という。

<details><summary></summary><div>

図 3.1 の PAD における後退代入が $(3.3)$ の手順である。
ここでも、 $m$ 組の定数項は `a(i,j)` の $j = n + 1, n + 2, \cdots , n + m$ 列にあり、かつ、それぞれの定数項のところに解 $x_j$ が求められる。
</div></details>

### 3.2.4 ピボット 選択

$(3.2)$ では、対角要素 $a_{kk}^{(k-1)}$ で割算をしている。
もし対角要素が $0$ であればこの割算はできないし、 $0$ でなくても非常に (絶対値の) 小さな数であれば、 $a_{kj}^{(k)}$ および $b_k{(k)}$ が異常に大きくなり、その結果 $a_{ij}^{(k)}$ および $b_i^{(k)}$ の右辺の第1項と第2項の大きさが極端に違ってきて、小さい第1項は大きい第2項によって丸めの誤差として無視されてしまう。
この丸めの誤差を小さく抑えるためには、割算をする要素の値が大きいことが必要である。
この要素のことを、ピボット (pivot － 枢軸、<ruby>要<rp>(</rp><rt>かなめ</rt><rp>)</rp></ruby> ) という。
$(3.2)$ ではピボットとしては対角要素を選んだが、できるだけ大きい要素を選ぶことが必要である。
ピボットとしてできるだけ大きい要素を選ぶことをピボット選択 (pivotting) という。

すでに消去が終わっている未知数の係数をピボットとして選ぶことはできない。
ピボット選択の範囲は $x_{k-1}$ まで消去できているときは、 $a_{ij}^{(k-1)}$ のうち $i \geqq k,\ j \geqq k$ の範囲の中から選択することになる。
この $(n - k + 1)^2$ 個の要素の全範囲から選択することを完全ピボット選択 (complete pivotting) という。
これに対して第 $k$ 列の中、すなわち、 $i \geqq k,\ j = k$ の中から選択することを部分ピボット選択 (partial pivotting) という。

ピボットとして $a_{ij}^{(k-1)}$ を選択したとき、それが第 $k$ 列にあるとき $(j = k)$ 、第 $i$ 番目の方程式と第 $k$ 番目の方程式と入れ換えてやることが必要である。
方程式の順番の入れ換えによっては解は変わらないから、入れ換えた後の手順の変更はいらない。
一方 $j \neq k$ の時は、未知数 $x_k$ と $x_j$ との入れ換えが必要になる。
したがって、完全ピボット選択を行ったときは、解を求めた後、未知数の順番をもとに戻してやらねばならない。

完全ピボット選択の方が、解の精度がよいことは当然であるが、選択の範囲が広いために手数がかかり、また未知数の順番を戻す必要があるため、小さな連立1次方程式は部分ピボット選択で済ませる。
大きな連立1次方程式の場合は直接法は用いず、反復法を用いることにする。

## 3.3 $\text{LU}$ 分解法

### 3.3.1 $\text{LU}$ 分解法

$n$ 次連立1次方程式 $(3.1)$ は

$$
Ax = b \tag{3.4}
$$

と書ける。
ただし $A$ は $n$ 次正方行列で、

$$
A = \begin{pmatrix}
    a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
    a_{21} & a_{22} & a_{23} & \cdots & a_{2n} \\
    a_{31} & a_{32} & a_{33} & \cdots & a_{2n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    a_{n1} & a_{n2} & a_{n3} & \cdots & a_{nn} \\
\end{pmatrix}
$$

いま、 $A$ を

$$
A = LU \tag{3.5}
$$

と分解したとしよう。
ここに $L$ は下三角行列、 $U$ は対角要素が $1$ である上三角行列である：

$$
L = \begin{pmatrix}
    l_{11} \\
    l_{21} & l_{22} & & \large{0} \\
    l_{31} & l_{32} & l_{33} \\
    \vdots & \vdots & \vdots & \ddots \\
    l_{n1} & l_{n2} & l_{n3} & \cdots & l_{nn} \\
\end{pmatrix}, \quad
U = \begin{pmatrix}
    1 & u_{12} & u_{13} & \cdots & u_{1n} \\
    & 1 & u_{23} & \cdots & u_{2n} \\
    & & 1 & \cdots & u_{3n} \\
    & \large{0} & & \ddots & \vdots \\
    & & & & 1 \\
\end{pmatrix}
$$

この分解を $\text{LU}$ 分解という<sup>[$2)$](#note2)</sup> 。

> <small id="note1">$^{2)}$ このように $U$ の対角要素を $1$ とする方法をドウリトル (Doolittle) 法、 $L$ の対角要素を $1$ とする方法をクラウト (Crout) 法と呼ぶ。
> このどちらの方法も $\text{LU}$ 分解は一意的である。
> ガウスの消去法との関連が明らかなため、ここではドウリトル法について述べる。</small>

行列 $A$ を $\text{LU}$ 分解できたら、この2つの行列 $L$ と $U$ とを用いて、 $(3.4)$ と $(3.5)$から $x$ を簡単な手順によって求めることが出来る。
すなわち、 $y = U x$ と置くと、 $(3.4)$ は2つの連立1次方程式

$$
L y = b \tag{3.6}
$$

$$
U x = y \tag{3.7}
$$

に分けられる。
$(3.6)$ より $y$ を求め、この $y$ を $(3.7)$ に代入して $x$ を求めれば、 $(3.4)$ の解 $x$ が求められる。

$(3.6)$ と $(3.7)$ を解くことは、 $L$ と $U$ が三角行列であるために容易である。
実際 $(3.6)$ を要素ごとに第1式から第 $n$ 式の順に書くと

$$
\begin{array}{lll}
l_{11} y_1 = b_1 & \therefore & y_1 = b_1 / l_{11} \\
l_{21} y_1 + l_{22} y_2 = b_2 & \therefore & y_2 = (b_2 - l_{21} y_1) / l_{22} \\
\qquad \cdots & & \qquad \cdots \\
\sum\limits_{j=1}^{n-1} l_{nj} y_j + l_{nn} y_n = b_n & \therefore & y_n = \left(
b_n - \sum\limits_{j=1}^{n-1} l_{nj} y_j \right) / l_{nn} \\
\end{array}
$$

と $y_1, y_2, \cdots , y_n$ の順に次々と求めることが出来る。
これを、前進代入という。

次に、 $(3.7)$ を第 $n$ 式から第1式の順に書いて、 $x_n$ から $x_1$ を逆順に求めることが出来る。
すなわち

$$
\begin{array}{lll}
x_n = y_n & \therefore & x_n = y_n \\
x_{n-1} + u_{n-1n} x_n = y_{n-1} & \therefore & x_{n-1} = y_{n-1} - u_{n-1n} x_n \\
\qquad \cdots & & \qquad \cdots \\
x_1 + \sum\limits_{j=2}^n u_{1j} x_j = y_n & \therefore & x_1 = y_n - \sum\limits_{j=2}^n u_{1j} x_j \\
\end{array}
$$

である。
これを、後退代入という。

$A$ を $\text{LU}$ 分解し、前進代入と後退代入によって解 $x$ を求める解法を $\text{LU}$ 分解法という。

### 3.3.2 $\text{LU}$ 分解

$\text{LU}$ 分解の手順は、前節のガウスの消去法の前進消去の手順と同じであることを示す。
そのために、ガウスの消去法の前進消去の手順 $(3.2)$ を行列 $G$ で表すと、

$$
GA = U =
\begin{pmatrix}
 1 & a_{12}^{(1)} & a_{13}^{(1)} & \cdots & a_{1n}^{(1)} \\
 & 1 & a_{23}^{(2)} & \cdots & a_{2n}^{(2)} \\
 & & 1 & \cdots & a_{3n}^{(3)} \\
 & \large{0} & & \ddots & \vdots \\
 & & & & 1
\end{pmatrix}
$$

であり、 $U$ は対角要素が $1$ である上三角行列であることに注意する ($(3.2’)$ を参照）。
この式から $A = G^{-1} U$ であるから、 $L = G^{-1}$ と置くと、 $\text{LU}$ 分解 $A = LU$
が得られる。

まず、 $G = G_n G_{n-1} \cdots G_k \cdots G_2 G_1$ と置き、 $k$ 回目の前進消去 $(3.2)$ を

$$
a_{ij}^{(k)} =
\left\{
\begin{array}{lll}
    a_{kj}^{(k-1)} / a_{kk}^{(k-1)} & i = k, & j \geqq k \\
    a_{ij}^{(k-1)} - a_{ik}^{(k-1)} a_{kj}^{(k)} & i > k, & j \geqq k \\
\end{array}
\right.
$$

と書き、これを $G_k$ で表すと、

$$
G_k =
\begin{pmatrix}
    1 \\
    & \ddots & & & & \large{0} \\
    & & 1 \\
    & & & 1 \\
    & & & -a_{k+1k}^{(k-1)} & 1 \\
    & \large{0} & & \vdots & & \ddots \\
    & & & -a_{nk}^{(k-1)} & & & 1 \\
\end{pmatrix}
\begin{pmatrix}
    1 \\
    & \ddots & & & & \large{0} \\
    & & 1 \\
    & & & 1/a_{kk}^{(k-1)} \\
    & & & & 1 \\
    & \large{0} & & & & \ddots \\
    & & & & & & 1 \\
\end{pmatrix}
$$

である。したがって、 $G_k^{-1}$ は<sup>[$3)$](#note3)</sup>

$$
\begin{aligned}
G_k^{-1} & =
\begin{pmatrix}
    1 \\
    & \ddots & & & & \large{0} \\
    & & 1 \\
    & & & a_{kk}^{(k-1)} \\
    & & & & 1 \\
    & \large{0} & & & & \ddots \\
    & & & & & & 1 \\
\end{pmatrix}
\begin{pmatrix}
    1 \\
    & \ddots & & & & \large{0} \\
    & & 1 \\
    & & & 1 \\
    & & & a_{k+1k}^{(k-1)} & 1 \\
    & \large{0} & & \vdots & & \ddots \\
    & & & a_{nk}^{(k-1)} & & & 1 \\
\end{pmatrix} \\
& =
\begin{pmatrix}
    1 \\
    & \ddots & & & & \large{0} \\
    & & 1 \\
    & & & a_{kk}^{(k-1)} \\
    & & & a_{k+1k}^{(k-1)} & 1 \\
    & \large{0} & & \vdots & & \ddots \\
    & & & a_{nk}^{(k-1)} & & & 1 \\
\end{pmatrix}
\end{aligned}
$$

である。

> <small id="note3">$^{3)}$ $A, B$ を $n$ 次正方行列とすると、 $(AB)^{-1} = B^{-1} A^{-1}$。</small>

$L = G^{-1} = G_1^{-1} G_2^{-1} \cdots G_k^{-1} \cdots G_{n-1}^{-1} G_n^{-1}$ は $G_k^{-1} (k = 1, \cdots n)$ の積であるから、

$$
\begin{aligned}
G_n^{-1} & =
\begin{pmatrix}
    1 \\
    & \ddots & & & \large{0} \\
    & & 1 \\
    & & & 1 \\
    & \large{0} & & & 1 \\
    & & & & & a_{nn}^{(n-1)} \\
\end{pmatrix}
\\ \\
G_{n-1}^{-1} G_n^{-1} & =
\begin{pmatrix}
    1 \\
    & \ddots & & & \large{0} \\
    & & 1 \\
    & & & 1 \\
    & \large{0} & & & a_{n-1n-1}^{(n-2)} \\
    & & & & a_{nn-1}^{(n-2)} & 1 \\
\end{pmatrix} G_n^{-1} \\
& =
\begin{pmatrix}
    1 \\
    & \ddots & & & \large{0} \\
    & & 1 \\
    & & & 1 \\
    & \large{0} & & & a_{n-1n-1}^{(n-2)} \\
    & & & & a_{nn-1}^{(n-2)} & a_{nn}^{(n-1)} \\
\end{pmatrix}
\\ \\
G_{n-2}^{-1} ( G_{n-1}^{-1} G_{n}^{-1} ) & =
\begin{pmatrix}
    1 \\
    & \ddots & & & \large{0} \\
    & & 1 \\
    & & & a_{n-2n-2}^{(n-3)} \\
    & \large{0} & &a_{n-1n-2}^{(n-3)} & 1 \\
    & & & a_{nn-2}^{(n-3)} & & 1 \\
\end{pmatrix}
\begin{pmatrix}
    1 \\
    & \ddots & & & \large{0} \\
    & & 1 \\
    & & & 1 \\
    & \large{0} & & & a_{n-1n-1}^{(n-2)} \\
    & & & & a_{nn-1}^{(n-2)} & a_{nn}^{(n-1)} \\
\end{pmatrix} \\
& =
\begin{pmatrix}
    1 \\
    & \ddots & & & \large{0} \\
    & & 1 \\
    & & & a_{n-2n-2}^{(n-3)} \\
    & \large{0} & &a_{n-1n-2}^{(n-3)} & a_{n-1n-1}^{(n-2)} \\
    & & & a_{nn-2}^{(n-3)} & a_{nn-1}^{(n-2)} & a_{nn}^{(n-1)} \\
\end{pmatrix} \\ \\
& \qquad \cdots \, \cdots \, \cdots \, \cdots
\end{aligned}
$$

と繰り返していけば

$$
\begin{aligned}
L & = G^{-1} = G_1^{-1} ( G_2^{-1} \cdots G_k^{-1} \cdots G_{n-1}^{-1} G_n^{-1} ) \\
& =
\begin{pmatrix}
    a_{11} \\
    a_{21} & 1 & & & \large{0} \\
    a_{31} & & 1 \\
    \vdots & & & \ddots \\
    \vdots & & & & \ddots \\
    a_{n1} & & & & & 1 \\
\end{pmatrix}
\begin{pmatrix}
    1 \\
    & a_{22}^{(1)} & & & \large{0} \\
    & a_{32}^{(1)} & a_{33}^{(2)} \\
    & \vdots & & \ddots \\
    & \vdots & & & \ddots \\
    & a_{n2}^{(1)} & a_{n3}^{(2)} & \cdots & \cdots & a_{nn}^{(n-1)} \\
\end{pmatrix} \\
& =
\begin{pmatrix}
    a_{11} \\
    a_{21} &　a_{22}^{(1)} & & & \large{0} \\
    a_{31} &　a_{32}^{(1)} & a_{33}^{(2)} \\
    \vdots &　\vdots & & \ddots \\
    a_{n1} &　a_{n2}^{(1)} & a_{n3}^{(2)} & \cdots & a_{nn}^{(n-1)} \\
\end{pmatrix}
\end{aligned}
$$

が得られる。
$L = G^{-1} (l_{ij} = a_{ij}^{(j-1)} )$ は求める下三角行列である。

$L$ の各要素は、 $(3.2)$ において $a_{kk}^{(k)} = 1, a_{ik}^{(k)} = 0 (i > k)$ とする直前の値である。
<details><summary></summary><div>

図 3.1 のガウスの消去法の PAD の［前進消去］の手順では $1$ あるいは $0$ にする演算はしていないから、この値は保存されている。
すなわち、ガウスの消去法の PAD 図 3.1 の ［前進消去］ の手順が完了したときには、行列 $A$ は次のようになっている：

$$
\begin{pmatrix}
    a_{11} & a_{12}^{(1)} & a_{13}^{(1)} & \cdots & a_{1n}^{(1)} \\
    a_{21} & a_{22}^{(1)} & a_{23}^{(2)} & \cdots & a_{2n}^{(2)} \\
    a_{31} & a_{32}^{(1)} & a_{33}^{(2)} & \cdots & a_{3n}^{(3)} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    a_{n1} & a_{n2}^{(1)} & a_{n3}^{(2)} & \cdots & a_{nn}^{(n-1)} \\
\end{pmatrix}
=
\begin{pmatrix}
l_{11} & u_{12} & u_{13} & \cdots & u_{1n} \\
l_{21} & l_{22} & u_{23} & \cdots & u_{2n} \\
l_{31} & l_{32} & l_{33} & \cdots & u_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
l_{n1} & l_{n2} & l_{n3} & \cdots & l_{nn} \\
\end{pmatrix}
$$

ガウスの消去法においては、このうち $u_{ij}$ を用いて後退代入を行って解 $x$ を求めた。
一方、 $\text{LU}$ 分解法においては、 $l_{ij}$ を用いて前進代入を行い、 $u_{ij}$ を用いて後退代入を行って解 $x$ を求めるのである。

以上の $\text{LU}$ 分解の手順を PAD 図 3.2 の［ $\text{LU}$ 分解］に示す。
この手順は、ガウスの消去法の［前進消去］の手順と同じである。
したがって、係数行列 $A$ は上のように変換されている。
</div></details>

### 3.3.3 ピボット選択

上述の $\text{LU}$ 分解の段階で、対角要素 $l_{kk} = a_{kk}^{(k-1)}$ で割算が必要である。
割算をするこの除数はガウスの消去法の前進消去のときのピボットと同じものである。
したがってピボット選択もガウスの消去法のときと同じ理由から必要である。

ガウスの消去法のときと異なるのは、ガウスの消去法の前進消去では右辺の定数項も対象になったが、 $\text{LU}$ 分解のときには係数行列だけが対象になっている。
だから、 $\text{LU}$ 分解のピボット選択で方程式の入れ換えを行ったら、その順番を記憶しておいて、前進代入にはいる前に、定数項の順番の入れ換えを行う必要がある。
そのためには、 $\text{LU}$ 分解前に $m_i$ <!-- ( PAD では配列 `mm(i)` ) --> に $m_i = i$ <!-- ( `mm(i)=i` ) --> としておいて、ピボット選択のため $k$ 行と $l$ 行を入れ換えるときには、 $m_k$ と $m_l$ の値も入れ換える。
こうすれば、 $\text{LU}$ 分解後の $i$ 行は $\text{LU}$ 分解前の $m_i$ 行であったことが記憶される。
こうして、前進代入では、まず $Ly = b$ の第 $k$ 行の $b_k$ としては $m_k$ を使って並べ変えた値 $b_{m_k}$ <!-- (`b(mm(k))`) --> を用いるわけである。

### 3.3.4 $\text{LU}$ 分解法の手順

<details><summary></summary><div>

以上の手順を PAD で表すと図 3.2 のようになる。
この PAD と図 3.1 とくらべると、［ $\text{LU}$ 分解］は、ガウスの消去法の［前進消去］において `m=0` と置いて得られるものと同じである。
また、［後退代入］は、 `m = 1` とすれば、二つの方法の後退代入は同じになる。
したがって、前進消去は $\text{LU}$ 分解と前進代入に分離されている。
$\text{LU}$ 分解は定数項 $b$ とは独立に行われるから、一度 $\text{LU}$ 分解を行っておけば、異なった $b$ に対してはそれぞれ前進代入と後退代入だけを行なって解 $x$ を求めることができる。

なお、 $l_{ij}$ と $u_{ij}$ はもとの行列要素 $a_{ij}$ に代入されているから、 $L$ と $U$ のために $A$ とは別の配列を用意しておく必要はない。
( 対角要素 $u_{ii}$ は $1$ であることがわかっているので、 $a_{ii}$ には $l_{ii}$ を記憶する。)
また、補助的に導入されたベクトル $y$ は不要で、 $y$ の代わりにいきなり $x$ を用いればよい。
( $y$ は $x$ に上書きされる。)
</div></details>

## 3.4 コレスキー分解法

### 3.4.1 コレスキー分解

実対称行列に対する $\text{LU}$ 分解法においては、$A = LU$ ( $L$ も $U$ も対角要素は $1$ とは限らない) と置くと、 $A^\top = A$ より

$$
(LU)^\top = U^\top L^\top = LU
$$

したがって $L = U^\top$ (同じことだが $U = L^\top$ ) であればよい。
ここでは $U = L^\top$ と置くと、実対称行列の分解は

$$
A = LL^\top \tag{3.8}
$$

となり、 $L$ を見いだす手順が決まると $U = L^\top$ は自動的に決まり、非対称行列の $\text{LU}$ 分解に比べて約半分の手順をふめば分解が出来上がることになる。
$A$ の要素を $a_{ii}$ 、 $L$ の要素を $l_{ij} (i \geqq j)$ と書く。
このとき $a_{ij} = a_{ji}$ であり、また $l_{ij} = 0 (i < j)$ である。

$(3.8)$ を要素ごとに書くと、 $l_{ij}$ を決めるには、 $i \geqq j$ の場合だけについて考えればよい。
すなわち

$$
\begin{aligned}
a_{jj} & = \sum\limits_{k=1}^{j-1} l_{jk}^2 + l_{jj}^2 \\
a_{ij} & = \sum\limits_{k=1}^{j-1} l_{ik} l_{jk} + l_{ij} l_{jj} & (i > j)
\end{aligned}
$$

故に、 $j = 1, 2, \cdots, n$ の順に、第 $j$ 列は

$$
\begin{array}{l}
\text{コレスキー分解 (平方根法)} \\
\begin{aligned}
l_{jj} & = \sqrt{ a_{jj} - \sum\limits_{k=1}^{j-1} l_{jk}^2 } \\
l_{ij} & = \left. \left( a_{ij} - \sum\limits_{k=1}^{j-1} l_{ik} l_{jk} \right) \right/ l_{jj} & (i > j)
\end{aligned} \end{array} \tag{3.9}
$$

と $l_{ij} (i \geqq j)$ を決定できる。
この分解はコレスキー (Cholesky) 分解と呼ばれている。
対角要素 $l_{jj}$ を決めるのに平方根を必要とするので、平方根法とも呼ばれる。

コレスキー分解がすんだら、 $Ly = b$ を前進代入で解き、次いで $L^\top x = y$ を後退代入で解く。

この方法は、平方根の中が負になったら実数計算の範囲では計算が出来ない。
また平方根の中が $0$ になったら、対角要素 $l_{jj}$ が $0$ になり、以後の計算は $l_{jj}$ による割り算があるため不可能になる。
係数行列 $A$ が対称正定値行列のときには $0$ や負になることはないが、正定値でない対称行列でも使えるというので、次に述べる改訂コレスキー法の方がよく使われる。

3.4.2 改訂コレスキー分解
実対称行列 $A$ を $A = LU = LDL^\top$ すなわち

$$
U = DL^\top \tag{3.10}
$$

と分解して、連立1次方程式を解く方法を、修正コレスキー法または改訂コレスキー法 (modified Cholesky method) と呼ぶ。
ここに、 $L$ は対角要素が $1$ である下三角行列、 $D$ は対角行列である。
$(3.10)$ の分解の手順を見出そう。

$U$ の $i$ 行 $j$ 列の要素を $u_{ij}$ と書く。
すなわち

$$
U =
\begin{pmatrix}
    u_{11} & u_{12} & u_{13} & \cdots & u_{1n} \\
    & u_{22} & u_{23} & \cdots & u_{2n} \\
    & & u_{33} & \cdots & u_{3n} \\
    & \LARGE{0} & & \ddots & \vdots \\
    & & & & u_{nn} \\
\end{pmatrix}
$$

一方、

$$
\begin{aligned}
DL^\top & =
\begin{pmatrix}
    d_1 \\
    & d_2 & & \LARGE{0} \\
    & & \ddots \\
    & \LARGE{0} & & \ddots \\
    & & & & d_n \\
\end{pmatrix}
\begin{pmatrix}
    1 & l_{21} & l_{31} & \cdots & l_{n1} \\
    & 1 & l_{32} & \cdots & l_{n2} \\
    & & 1 & \cdots & l_{n3} \\
    & \LARGE{0} & & \ddots & \vdots \\
    & & & & 1 \\
\end{pmatrix} \\
& =
\begin{pmatrix}
    d_1 & l_{21} d_1 & l31 d_1 & \cdots & l_{n1} d_1 \\
    & d_2 & l_{32} d2 & \cdots & l_{n2} d_2 \\
    & & d_3 & \cdots & l_{n3} d_3 \\
    & \LARGE{0} & & \ddots & \vdots \\
    & & & & d_n \\
\end{pmatrix}
\end{aligned}
$$

であるから

$$
u_{ii} = d_i, \quad u_{ij} = l_{ji}d_i (i < j) \tag{3.11}
$$

である。
そうして、

$$
A = LU =
\begin{pmatrix}
    1 \\
    l_{21} & 1 \\
    l_{31} & l_{32} & 1 \\
    \vdots & \vdots & \vdots & \ddots \\
    l_{n1} & l_{n2} & l_{n3} & \cdots & 1 \\
\end{pmatrix}
\begin{pmatrix}
    d_1 & u_{12} & u_{13} & \cdots & u_{1n} \\
    & d_2 & u_{23} & \cdots & u_{2n} \\
    & & d_3 & \cdots & u_{3n} \\
    & & & \ddots & \vdots \\
    & & & & d_n \\
\end{pmatrix}
$$

より、各要素を 行順 $k = 1, 2, \cdots , n$ に書き下す。
$k$ 行は、

$$
\begin{matrix}
i < k & a_{ki} = \sum\limits_{j=1}^{i-1} l_{kj} u_{ji} + u_{ik} & \therefore u_{ik} = a_{ki} - \sum\limits_{j=1}^{i-1} l_{kj} u_{ji}, & l_{ki} = u_{ik} / d_i \\
i = k & a_{kk} = \sum\limits_{j=1}^{k-1} l_{kj} u_{jk} + d_k & \therefore d_k = a_{kk} - \sum\limits_{j=1}^{k-1} l_{kj} u_{jk} &
\end{matrix}
$$

この第１式 $(i < k)$ の形は、両辺の $u_{ik}$ と $u_{ji}$ の決まる順が見にくいので、

$$
l_{kj} u_{ji} = ( l_{ij} d_j ) l_{kj} = l_{ij} ( l_{kj} d_j ) = l_{ij} u_{jk}
$$

と添え字の $i$ と $k$ の付け替えて、

$$
u_{ik} = a_{ki} - \sum\limits_{j=1}^{i-1} l_{ij} u_{jk}, \quad \therefore l_{ki} = u_{ik}/d_i
$$

とする。
明らかに、 $j < i$ であるから、右辺の $u_{jk}$ は左辺の $u_{ik}$ より先に決まる。

こうして、改訂コレスキー分解は $k = 1, 2, \cdots , n$ の順に

$$
\begin{array}{}
\text{改訂コレスキー分解} \\
\begin{array}{l}
u_{ik} = a_{ki} - \sum\limits_{j=1}^{i-1} l_{ij} u_{jk} \\
l_{ki} = u_{ik} / d_i
\end{array} & (i < k) \\
d_k = a_{kk} - \sum\limits_{j=1}^{k-1} l_{kj} u_{jk}
\end{array} \tag{3.12}
$$

となる。

ここで最終的に必要なのは $l_{ki}$ と $d_k$ であるから、 $u_{ik}$ は作業用の配列であると見なせる。
この配列は、ある $k$ で用いたらその値は他の $k$ では使わないから、 $u_{ik}$ は $u_i$ と書いてよい。

対角要素 $d_k$ は、負になってもかまわないが、 $0$ になったら困る。
$d_k$ はガウスの消去法や LU 分解法のピボットと同じものである。
そこでピボット選択が必要となる。
ただし、対称性をこわさないためには、行の入れ替えだけではなく、列の入れ替えも行なう必要がある。

<details><summary></summary><div>

改訂コレスキー分解法の手順を図 3.3 の PAD に示す。
改訂コレスキー分解の結果は元の行列 $A$ の配列を用いることが出来る。
すなわち、 $l_{ij}$ は $a_{ij}$ に、 $d_j$ は $a_{jj}$ に記憶させればよい。
PAD 中で `D(j)` は $a_{jj}$ 、 `L(i,j)` は $a_{ij}$ である。
このときも `L` の対角要素 $l_{ii} = 1$ は記憶する必要はない。
$A$ の非対角要素の上半分の $a_{ij} (i < j)$ は、元の値がそのまま保存されている。
また、 $k = 1$ のときは $a_{11} = d_1$ なので、 $k = 2$ から始める。
言うまでもなく、 `x(i), y(i), b(i)` は同一記憶領域に格納することができる。
</div></details>

## 3.5 ベクト ルのノルム、行列のノルム、条件数

### 3.5.1 ベクトルのノルム

連立1次方程式の反復解法にはいる前に、ベクトルおよび行列のノルムについて述べよう。
ノルム (norm) とはベクトルや行列の「大きさ」を表す量のことである。

スカラーの場合は、絶対値で大きさを言い表すことができる。
しかし、ベクトルと行列は、複数の成分や要素を持つ。
これら複数の量をまとめた一つの量で大きさを言い表そうというわけである。

まず、ベクトルのノルムについて考える。
ベクトル $x$ のノルムを $\Vert x \Vert$ と書く。
ベクトルのノルムは次の3つの条件を満たすものなら何でもよい。

$$
\begin{aligned}
(\text{i}) & \quad x \neq 0 ならば常に \Vert x \Vert > 0、x = 0 のときにかぎり \Vert x \Vert = 0 \\
(\text{ii}) & \quad \Vert \alpha x \Vert = \vert \alpha \vert \Vert x \Vert \quad (ただし \alpha は複素数) \\
(\text{iii}) & \quad \Vert x + y \Vert \leqq \Vert x \Vert + \Vert y \Vert \\
\end{aligned}
$$

このようなノルムとして、 $n$ 次元ベクトル $x = (x_1 x_2 \cdots x_n)^\top$ のとき

$$
\Vert x \Vert p \equiv \left( \sum_{i=1}^n \vert x_i \vert ^p \right)^{1/p} \quad (p ノルム) \tag{3.13}
$$

がある。
これを $p$ ノルムという。
$n = 1$ の場合はスカラーの絶対値に対応する。
よく用いられるのは $1$ ノルム

$$
 \Vert x \Vert _1 = \sum_{i=1}^n \vert x_i \vert \qquad (1 ノルム) \tag{3.14}
$$

であり、マンハッタン ノルム (Manhattan norm) ともいう。
また $2$ ノルム

$$
\Vert x \Vert _2 = \left( \sum_{i=1}^n \vert x_i \vert ^2 \right) ^{1/2} \qquad (ユークリッド ノルム) \tag{3.15}
$$

はピタゴラスの定理に出てくるベクトルの長さで、ベクトルのユークリッド ノルム (Euclid norm) という。
$p \to \infty$ の極限値を考えて、無限大ノルムである

$$
\Vert x \Vert ∞ = \lim_{p \to \infty} \left( \sum_{i=1}^n \vert x_i \vert ^p \right) ^{1/p} = \max_i \vert x_i \vert (最大ノルム) \tag{3.16}
$$

を最大ノルムまたは最大値ノルム (maximum norm) という。
よく使われるのは、以上の $\Vert x \Vert _1, \Vert x \Vert _2, \Vert x \Vert _\infty$ の3つである。
とくに $\Vert x \Vert _\infty$ は数値計算が簡単なのでよく用いられる。

> #### ［例 3.1］ ベクトルのノルムの簡単な例
> 
> $x = (\ 1 \quad 3 \quad 4 \quad 5 \ )^\top$ のとき
> 
> $$
> \begin{array}{ll}
> (1) & \Vert x \Vert _1 = 1 + 3 + 4 + 5 = 13 \\
> (2) & \Vert x \Vert _2 = \sqrt{1^2 + 3^2 + 4^2 + 5^2} = 7.141428 \cdots \\
> (3) & \Vert x \Vert _\infty = \max(1, 3, 4, 5) = 5 \\
> \end{array}
> $$

3.5.2 行列のノルム
行列 $A$ のノルム $ \Vert A \Vert $ は、ベクトルのノルム $ \Vert Ax \Vert $ と $ \Vert x \Vert $ を用いて、

$$
 \Vert A \Vert \equiv \max_x \frac{\Vert Ax \Vert}{\Vert x \Vert} \qquad \text{(自然な行列ノルム)} \tag{3.17}
$$

として定義しよう。
この式の右辺の意味は、　$x \neq 0$ であるようなあらゆる $x$ を用いて右辺の分数が最大になったときの値をノルム $ \Vert A \Vert $ とするということである。
この定義から、ノルム $ \Vert A \Vert $ はつぎの性質がある (何故か)。
ただし、 $A$ と $B$ はともに行列である。

$$
\begin{array}{rl}
(\text{iv}) & \Vert Ax \Vert \leqq \Vert A \Vert \Vert x \Vert \\
(\text{v}) & \Vert AB \Vert \leqq \Vert A \Vert \Vert B \Vert \\
(\text{vi}) & \Vert A + B \Vert \leqq \Vert A \Vert + \Vert B \Vert \\
\end{array}
$$

行列のノルム $ \Vert A \Vert $ は、 $(3.17)$ の右辺のベクトルのノルムの選び方によって、$\Vert A \Vert _1$ 、 $\Vert A \Vert _2$ 、 $\Vert A \Vert_\infty$ などの異なった値を持つ。
たとえば $A$ の要素を $a_{ij}$ とすれば、ベクトルのノルムを1ノルム、ユークリッド ノルム、最大ノルムにとれば

$$
\Vert A \Vert _1 = \max_j \left\{ \sum_{i=1}^n \vert a_{ij} \vert \right\} \qquad (絶対値の列和の最大) \tag{3.18}
$$

$$
\Vert A \Vert _2 = \sqrt{ \rho(A^\top A)} \qquad ( A^\top A のスペクトル半径の平方根) \tag{3.19}
$$

$$
\Vert A \Vert _\infty = \max_i \left\{ \sum_{j=1}^n \vert a_{ij} \vert \right\} \qquad (絶対値の行和の最大) \tag{3.20}
$$

が得られる (証明は 問題 3-2 参照)。
ここに、 $\rho(A^\top A)$ は非負定値対称行列である $A^\top A$ のスペクトル半径である。
一般に $n$ 次正方行列のスペクトル半径 $\rho$ とは、固有値の絶対値の最大値で

$$
\rho \equiv \max \left\{ \vert \lambda_1 \vert , \vert \lambda_2 \vert , \cdots , \vert \lambda_n \vert \right\} \tag{3.21}
$$

と定義される。
この意味で、上の $ \Vert A \Vert _2$ を $A$ のスペクトル ノルムともいう。
特に、 $A$ が実対称行列 $(A^\top = A)$ であれば、

$$
\Vert A \Vert _2 = \rho(A) \qquad (実対称行列のスペクトル ノルム) \tag{3.22}
$$

である (問題 3-3 参照)。

以上の $(3.17)$ に属さない行列のノルムとしては、すべての要素の絶対値の2乗の和の平方根

$$
\Vert A \Vert _F = \Vert A \Vert _E \equiv \left( \sum_{i,j=1}^n \vert a_{ij} \vert ^2 \right)^{1/2} \tag{3.23}
$$

がある。
このノルムはフロベニウス (Frobenius) ノルムとよばれる。
ベクトルのユークリッド ノルム $(3.15)$ と似ているので、行列のユークリッド (Euclid) ノルムともよばれる。

> #### ［例 3.2］ 行列のノルムの簡単な例
>
> $A$ は単位行列 $I$ であるとき、自然な行列ノルムはすべて $1$ である。
>
> $$
> \Vert I \Vert = \max_x \frac{ \Vert Ix \Vert }{ \Vert x \Vert } = \max_x \frac{ \Vert x \Vert }{ \Vert x \Vert } = 1 \tag{3.24}
> $$
>
> $ A = \begin{pmatrix}
> 3 &  5  & -4 \\
> 2 & -1  &  8 \\
> 6 &  7  & -9 \\
> \end{pmatrix}$ とする。
> このとき、1ノルムと最大ノルムは
>
> $$
> \Vert A \Vert _1 = \max \{ 3 + 2 + 6, 5 + 1 + 7, 4 + 8 + 9 \} = \max \{ 11, 13, 21 \} = 21 \\
> \Vert A \Vert _\infty = \max \{ 3 + 5 + 4, 2 + 1 + 8, 6 + 7 + 9 \} = \max \{ 12, 11, 22 \} = 22
> $$
>
> 次に
>
> $$
> A^\top A =
> \begin{pmatrix}
>  49 &  55 & -50 \\
>  55 &  75 & -91 \\
> -50 & -91 & 161 \\
> \end{pmatrix}
> $$
>
> の固有値は
>
> $$
> \det(\lambda I - A^\top A) = \lambda^3 - 285 \lambda^2 + 9833 \lambda - 11881 = 0
> $$
>
> の解
>
> $$
> \lambda_1 = 245.075, \quad \lambda_2 = 38.6709, \quad \lambda_3 = 1.25363
> $$
>
> であるから、スペクトル半径 $\rho(A^\top A)$ は
>
> $$
> \rho(A^\top A) = \max \{ \vert \lambda_1 \vert , \vert \lambda_2 \vert , \vert \lambda_3 \vert \} = 245.075
> $$
>
> 故に
>
> $$
> \Vert A \Vert _2 = \sqrt{ \rho(A^\top A) } = \sqrt{245.075} = 15.665
> $$
>
> また、フロベニウス ノルム (ユークリッド ノルム) は
>
> $$
> \Vert A \Vert _F = \Vert A \Vert _E = \left( \sum_{i,j=1}^3 \vert a_{ij} \vert ^2 \right)^{1/2} = \sqrt{285} = 16.882
> $$

***

#### 例題 3.1

行列 $A$ のスペクトル半径を \rho とすれば、 $\rho \leqq \Vert A \Vert $ を示せ。

#### ［解］

$A$ の固有値と固有ベクトルをを $\lambda$ 、 $u$ とすれば、 $Au = \lambda u$ 。
行列のノルムの性質 $(\text{iv})$ から

$$
\Vert Au \Vert \leqq \Vert A \Vert \Vert u \Vert
$$

一方、ベクトルのノルムの性質 $(\text{ii})$ から

$$
\Vert \lambda u \Vert = \vert \lambda \vert \Vert u \Vert
$$

故に

$$
\vert \lambda \vert \leqq \Vert A \Vert
$$

これはすべての固有値に対して成り立つから、絶対値最大の固有値に対しても成り立ち、 $\rho \leqq \Vert A \Vert$ なお対称行列に対しては、ノルムとしてスペクトルノルムをとれば、等号が成り立つ。
(問題 3-3 参照)

***

#### 例題 3.2

$\Vert A \Vert < 1$ のとき、次の不等式が成り立つことを示せ。

$$
\frac{1}{1 + \Vert A \Vert} \leqq \Vert (I + A)^{-1} \Vert \leqq \frac{1}{1 - \Vert A \Vert} \tag{3.25}
$$

***

#### ［解］

$I = (I + A)^{-1} (I + A)$ より

$$
1 \leqq \Vert (I + A)^{-1} \Vert \cdot \Vert I + A \Vert \leqq \Vert (I + A)^{-1} \Vert (1 + \Vert A \Vert )
$$

故に

$$
\frac{1}{1 + \Vert A \Vert} \leqq \Vert (I + A)^{-1} \Vert 
$$

また、 $I = (I + A)(I + A)^{-1} = (I + A)^{-1} + A(I + A)^{-1}$ 、すなわち、 $(I + A)^{-1} = I - A(I + A)^{-1}$ より

$$
\Vert (I + A)^{-1} \Vert = \Vert I - A(I + A)^{-1} \Vert \leqq 1 + \Vert A \Vert \cdot \Vert (I + A)^{-1} \Vert 
$$

これを $\Vert (I + A)^{-1} \Vert$ について解けば

$$
\Vert (I + A)^{-1} \Vert \leqq \frac{1}{1 - \Vert A \Vert}
$$

***

### 3.5.3 条件数

$A$ を正則行列とする。2つのノルム $\Vert A \Vert$ と $\Vert A^{-1} \Vert$ の積を、行列 $A$ の条件数 (condition number) と呼び、

$$
\mathrm{cond}(A) \equiv \Vert A^{-1} \Vert \Vert A \Vert \tag{3.26}
$$

と書く。
$\mathrm{cond}(A)$ は、連立1次方程式の数値解の精度を推定する指標を与える重要な量である。
以下、このことを調べておこう。

連立1次方程式を $Ax = b$ とする。
実際の数値計算では、 $A$ にも $b$ にも誤差があるのが普通である。
$A$ の誤差を $\varDelta A$ 、 $b$ の誤差を $\varDelta b$ とする。
これらの誤差のため、実際に解いているのは、

$$
(A + \varDelta A)(x + \varDelta x) = b + \varDelta b \tag{3.27}
$$

なる方程式である。
ここに $\varDelta x$ は、 $\varDelta A$ や $\varDelta b$ のために生じる $x$ の誤差である。
これより、 $x$ の誤差のノルム $\Vert \varDelta x \Vert$ の限界を推定する。
$(3.27)$ より $\varDelta x$ を求めると、

$$
\varDelta x = (I + A^{-1} \varDelta A)^{-1} A^{-1}(\varDelta b - \varDelta A \cdot x)
$$

故に

$$
\begin{aligned}
\Vert \varDelta x \Vert & = \Vert (I + A^{-1} \varDelta A)^{-1} A^{-1} (\varDelta b - \varDelta A \cdot x) \Vert \\
& \leqq \Vert (I + A^{-1} \varDelta A)^{-1} \Vert \cdot \Vert A^{-1} \Vert ( \Vert \varDelta b \Vert + \Vert \varDelta A \Vert \cdot \Vert x \Vert )
\end{aligned}
$$

ここで、 $\Vert A^{-1} \varDelta A \Vert < 1$ とすると、 $(3.25)$ より

$$
\Vert(I + A^{-1} \varDelta A)^{-1} \Vert \leqq \frac{1}{1 - \Vert A^{-1} \varDelta A \Vert } \leqq \frac{1}{1 - \Vert A^{-1} \Vert \cdot \Vert \varDelta A \Vert }
$$

であり、また $\Vert b \Vert \leqq \Vert A \Vert \cdot \Vert x \Vert $ より

$$
\Vert \varDelta b \Vert + \Vert \varDelta A \Vert \cdot \Vert x \Vert \leqq
\left( \frac{\Vert \varDelta b \Vert }{ \Vert b \Vert } + \frac{\Vert \varDelta A \Vert }{ \Vert A \Vert } \right) \Vert A \Vert \cdot \Vert x \Vert
$$

これらを代入すると、

$$
\frac{\Vert \varDelta x \Vert}{\Vert x \Vert } \leqq \frac{ \mathrm{cond}(A) }
{1 - \mathrm{cond}(A) ( \Vert \varDelta A \Vert / \Vert A \Vert )}
\left( \frac{ \Vert \varDelta A \Vert }{ \Vert A \Vert } + \frac{ \Vert \varDelta b \Vert}{ \Vert b \Vert } \right)
\tag{3.28}
$$

である。
この式は、 $\mathrm{cond}(A)$ が大きい問題では、誤差が大きくなる可能性があることを示している。
$\mathrm{cond}(A)$ が大きい行列は悪条件 (ill-conditioned) の行列という。

## 3.6 ヤコビ法

### 3.6.1 ヤコビ 法の公式

まず、連立1次方程式 $(3.1)$ の第 $i$ 式を $x_i$ について解く。
そうして、右辺の $x_j$ の肩に添え字 $(k)$ 、左辺の $x_i$ の肩に添え字 $(k + 1)$ をつければ、次のヤコビ法の公式を得る。

$$
\begin{array}{l}
\text{ヤコビ法 (1)} \\
x_i^{(k+1)} = \left. \left( b_i - \sum\limits_{j=1}^{i-1} a_{ij} x_j^{(k)} - \sum\limits_{j=i+1}^n a_{ij} x_j^{(k)} \right) \right/ a_{ii} \qquad (1 \leqq i \leqq n)
\end{array} \tag{3.29}
$$

$k = 0$ として初期値 $x_1^{(0)} , x_2^{(0)} , x_3^{(0)} , \cdots , x_n^{(0)}$ を適当に (出来れば解に近い値を) 与え、公式 $(3.29)$ によって $x_1^{(1)} , x_2^{(1)} , x_3^{(1)} , \cdots , x_n^{(1)}$ を求める。
次いで $k = 1$ とし て、再び公式 $(3.29)$ によって、 $x_1^{(2)} , x_2^{(2)} , x_3^{(2)} , \cdots , x_n^{(2)}$ を求める。
以下 $k = 2, 3, \cdots $ と反復繰り返して、 $i = 1, 2, 3, \cdots , n$ のすべての $x_i$ について $x_i^{(k)}$ と $x_i^{(k+1)}$ が必要な精度一致したとき収束したとして、そのときの $x_i^{(k+1)}$ を $x_i$ の求める解とする。
これがヤコビ法である。

残差 $r = b - Ax$ を用いてヤコビ法の公式を書き変えると、ややすっきりした公式が得られる。

$$
\begin{array}{l}
ヤコビ法 (2) \\
\quad x_i^{(k+1)} = x_i^{(k)} + r_i^{(k)} / a_{ii} \qquad (1 \leqq i \leqq n) \\
\qquad ただし \quad r_i^{(k)} = b_i - \sum\limits_{j=1}^n a_{ij} x_j^{(k)}
\end{array} \tag{3.30}
$$

<details><summary></summary><div>

2つのヤコビ法の手順を図 3.4 の PAD に示す。
PAD においては、求められた $x_i^{(k)}$ を `x0(i)` 、これから求める $x_i^{(k+1)}$ を `x(i)` と書いてある。
$k$ を1増やすとき `x0(i)=x(i)` と代入して、ヤコビ法の公式 $(3.29)$ または $(3.30)$ を適用している。
</div></details>

### 3.6.2 ヤコビ 法の収束性

ヤコビ法はいつも収束するとは限らない。
ヤコビ法の収束性を調べよう。
行列 $A$ を下三角行列 $L$ 、対角行列 $D$ 、上三角行列 $U$ の和に分解して

$$
A = L + D + U
$$

と書く。
ただし

$$
A =
\begin{pmatrix}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    a_{21} & a_{22} & \cdots & a_{2n} \\
    \vdots & \vdots &        & \vdots \\
    a_{n1} & a_{n2} & \cdots & a_{nn} \\
\end{pmatrix} , \quad
L =
\begin{pmatrix}
    0 & & & \Large{0} \\
    a_{21} & 0 \\
    \vdots & & \ddots \\
    a_{n1} & a_{n2} & \cdots & 0 \\
\end{pmatrix} \\ \quad \\
D =
\begin{pmatrix}
    a_{11} & & & \Large{0} \\
    & a_{22} \\
    & & \ddots \\
    \Large{0} & & & a_{nn} \\
\end{pmatrix} , \quad
U =
\begin{pmatrix}
    0 & a_{12} & \cdots & a_{1n} \\
    & 0 & \cdots & a_{2n} \\
    & & \ddots \\
    \Large{0} & & & 0 \\
\end{pmatrix}
$$

そうすると、 $(3.29)$ は

$$
\begin{aligned}
x^{(k+1)} & = D^{-1} [ b - (L + U )x^{(k)} ] \\
& = H_\text{J}x^{(k)} + c_\text{J} \tag{3.31}
\end{aligned}
$$

と表すことができる。
ここに

$$
\begin{aligned}
H_\text{J} & = -D^{-1}(L + U ) \qquad \text{(反復行列)} \\
c_\text{J} & = D^{-1}b
\end{aligned} \tag{3.32}
$$

であり、 $H_\text{J}$ はヤコビ法の反復行列と呼ばれる行列である。
ヤコビ法の収束性は反復行列 $H_\text{J}$ によって決まる。
いま $x^*$ に収束したとすると、 $(3.31)$ より

$$
\begin{aligned}
x^{(k+1)} - x^* = & H_\text{J} ( x^{(k)} - x^* ) \\
= & H_\text{J}^2 (x^{(k-1)} - x^*) \\
& \cdots \\
= & H_\text{J}^{k+1} (x^{(0)} - x^*) \\
\end{aligned} \tag{3.33}
$$

故に反復回数 $k$ が十分大きいときには、 $H_\text{J}$ のスペクトル半径 $\rho_\text{J}$ を使って、

$$
\Vert x^{(k+1)} - x^* \Vert \cong \rho_\text{J}^{k+1} \Vert x^{(0)} - x^* \Vert \tag{3.34}
$$

収束するためには $H_\text{J}$ のスペクトル半径 $\rho_\text{J}$ が

$$
\rho_\text{J} < 1 \tag{3.35}
$$

であることが必要十分である。
与えられた $H_\text{J}$ の $\rho_\text{J}$ を求めることは一般には困難であるが、

$$
\min_i \left\{ \sum\limits_{\substack{j = 1 \\ j \neq i}}^n \frac{ \vert a_{ij} \vert }{ \vert a_{ii} \vert } \right\}
\leqq \rho_\text{J} \leqq 
\max_i \left\{ \sum\limits_{\substack{j = 1 \\ j \neq i}}^n \frac{ \vert a_{ij} \vert }{ \vert a_{ii} \vert } \right\}
= \Vert H_\text{J} \Vert _\infty \tag{3.36}
$$

である程度の推定は出来る。
(この右辺は、すぐ後で示すように最大値ノルム $\Vert H_\text{J} \Vert _\infty$ である。) ここでは、 $\rho_\text{J}$ の代わりに $H_\text{J}$ のノルム $\Vert H_\text{J} \Vert$ を用いて収束するか否かの判定をしよう。
$\rho_\text{J}$ と $\Vert H_\text{J} \Vert$ は

$$
\rho_\text{J} \leqq \Vert H_\text{J} \Vert \tag{3.37}
$$

の関係にある。
ノルム $\Vert H_\text{J} \Vert$ にもいろいろあるが、その中でも最大値ノルムは容易に求められる。
$H_\text{J}$ の要素を $h_{ij}$ とすると、 $(3.32)$ より

$$
\begin{aligned}
& h_{ii} = 0, \\
& h_{ij} = -a_{ij} / a_{ii} \qquad (i \neq j)
\end{aligned}
$$

であるから、最大値ノルムは、定義により

$$
 \Vert H_\text{J} \Vert _\infty
 \equiv \max_i \left\{ \sum_{j=1}^n \vert h_{ij} \vert \right\}
= \max_i \left\{ \sum\limits_{\substack{j = 1 \\ j \neq i}}^n \frac{ \vert a_{ij} \vert }{ \vert a_{ii} \vert } \right\} \tag{3.38}
$$

この最大値ノルムをもちいて、収束条件は

$$
\begin{array}{l}
\text{反復法の収束十分条件} \\
\quad \max\limits_i \left\{ \sum\limits_{\substack{j = 1 \\ j \neq i}}^n \displaystyle\frac{ \vert a_{ij} \vert }{ \vert a_{ii} \vert } \right\} < 1
\end{array} \tag{3.39}
$$

と書ける。
$(3.39)$ を満足する行列 $A$ は、対角要素の絶対値は非対角要素の絶対値の和より大きいので、狭義優対角行列という。
$(3.39)$ の $<$ を $\leqq$ で置き換えたときの行列を (広義) 優対角行列という。
$(3.37)$ よりこの条件は十分条件であり、この条件を満たせば必ず収束する ( $\rho_\text{J} \leqq \Vert H_\text{J} \Vert < 1$ )。
しかし、この条件を満たさなくても収束することはある ($\rho_\text{J} \leqq 1 \leqq \Vert H_\text{J} \Vert$ )。
ただし、満たさないときには、収束するにしても反復回数は大きくなることが多い (例外はある)。

反復回数 $N$ は、許容相対誤差を $\varepsilon_R$ とすると $\Vert x^{(N)} - x^* \Vert \cong \varepsilon_R \Vert x^* \Vert$ なる $N$ である。
$\Vert x^{(0)} - x^* \Vert \cong \varepsilon_0 \Vert x^* \Vert \cong \Vert x^* \Vert$ とすると、 $(3.34)$ より $\varepsilon_R \cong \rho_\text{J}^N$ 、故に $N$ は

$$
\begin{array}{l}
推定反復回数 \\
\quad N = \displaystyle\frac{\log \varepsilon_R}{\log \rho_\text{J}}
\tag{3.40}
\end{array}
$$

と推定される。
$\rho_\text{J}$ を求めるのが困難な時には、最大反復回数 $N_\text{max}$ を

$$
\begin{array}{l}
最大反復回数 \\
\quad N_\text{max} = \displaystyle\frac{\log \varepsilon_R}{\log \Vert H_\text{J} \Vert } \tag{3.41}
\end{array}
$$

として、反復回数は $N_\text{max}$ 以下におさえる。
$(3.37)$ より $N \leqq N_\text{max}$ である。

ヤコビ法以外の反復法の多くは、反復行列やそのスペクトル半径は簡単に求められないので、 $(3.39)$ の判定は、ヤコビ法に限らず一般に反復法の収束十分条件として用いる。
推定反復回数と最大反復回数も同様である。

## 3.7 ガウス・ザイデル法

### 3.7.1 ガウス・ザイデル法の公式

ヤコビ法の公式 $(3.29)$ における右辺第1の総和 $(\sum)$ の $x_j^{(k)} (1 \leqq j \leqq i - 1)$ を、その段階ですでに求められている $x_j^{(k+1)} (1 \leqq j \leqq i - 1)$ で置き換えれば、次のガウス・ザイデル (Gauss-Seidel) 法の公式が得られる。

$$
\begin{array}{l}
\text{ガウス・ザイデル法 (1)} \\ \quad
x_i^{(k+1)} = \left( b_i - \sum\limits_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \sum\limits_{j=i+1}^na_{ij} x_j^{(k)} \right) / a_{ii} \qquad (1 \leqq i \leqq n)
\end{array} \tag{3.42}
$$

この公式は、右辺の $x_j$ はつねに最新の値を使っているから収束性の向上が期待され、古い値を使わないのでプログラムも書き易くなり、その結果効率もよくなる。

ガウス・ザイデル法の公式は次のようにも書ける：

$$
\begin{array}{l}
\text{ガウス・ザイデル法 (2)} \\
\quad x_i^{(k+1)} = x_i^{(k)} + r_i^{(k)} / a_{ii} \qquad (1 \leqq i \leqq n) \tag{3.43} \\
\quad \text{ただし} \quad r_i^{(k)} = b_i - \sum\limits_{j=1}^{i-1}a_{ij} x_j^{(k+1)} - \sum\limits_{j=i}^n a_{ij} x_j^{(k)}
\end{array}
$$

$r_i$ は残差である。
この形は、 $x_i^{(k)}$ を $r_i^{(k)} / a_{ii}$ だけ修正したものが $x_i^{(k+1)}$ であることを示し、次の SOR 法への準備であるが、実際にも効率のよい手順を与える。

<details><summary></summary><div>

図 3.5 に $(3.43)$ の手順の PAD を示す。
</div></details>

## 3.7.2 ガウス・ザイデル法の収束性

$(3.42)$ を前節の $L$ 、 $D$ 、 $U$ を用いて書き表すと

$$
\begin{aligned}
x^{(k+1)} = (D + L)^{-1} [ b - U x^{(k)} ]
= H_\text{GS}x^{(k)} + c_\text{GS}
\end{aligned} \tag{3.44}
$$

となる。
ここに

$$
\begin{align}
H_\text{GS} & = -(D + L)^{-1} U \qquad \text{(反復行列)} \tag{3.45} \\
c_\text{GS} & = (D + L)^{-1} b \tag{3.46} \\
\end{align}
$$

であり、 $H_\text{GS}$ はガウス・ザイデル法の反復行列である。
ヤコビ法と同様に、ガウス・ザイデル法の反復が収束するためには、 $H_\text{GS}$ のスペクトル半径 $\rho_\text{GS}$ が

$$
\rho_\text{GS} < 1
$$

であることが必要十分である。
このような行列 $A$ としては、 $(3.39)$ の収束条件を満足する狭義優対角行列と実対称正定値行列がある。
また、ヤコビ法の反復行列 $H_\text{J}$ が非負行列 (すべての要素が非負の行列) であるときには、 $H_\text{J}$ と $H_\text{GS}$ のスペクトル半径 $\rho_\text{J}$ と $\rho_\text{GS}$ の間には

$$
(1) \ \rho_\text{GS} = \rho_\text{J} = 0 \\
(2) \ \rho_\text{GS} = \rho_\text{J} = 1 \\
(3) \ \rho_\text{GS} < \rho_\text{J} < 1 \\
(4) \ \rho_\text{GS} > \rho_\text{J} > 1 \\
$$

のうちの1つの関係だけが成立している<sup>[$4)$](#note4)</sup> 。
このときにはヤコビ法が収束すればガウス・ザイデル法も収束し、ヤコビ法が収束しなければガウス・ザイデル法も収束しない。
かつ、収束する場合にはガウス・ザイデル法の方が早く収束する。

> <small id="note4">$^{4)}$ A. ラルストン, P. ラビノヴィッツ：数値解析の理論と応用, 下巻, 1986, ブレイン図書出版 (発売元 丸善)</small>

このことが、ヤコビ法よりガウス・ザイデル法がよく使われる根拠になっている。
したがって、ガウス・ザイデル法の収束条件は、ヤコビ法の収束条件 (十分条件) $(3.39)$
で見当をつけることでガマンする。

しかし、条件 $(3.39)$ はヤコビ法の十分条件であってガウス・ザイデル法の必要十分条件ではないから、この条件を満足しなくても収束することは有り得る。
またヤコビ法は収束するのにガウス・ザイデル法が収束しないこと $(\rho_\text{J} < 1 < \rho_\text{GS} )$ もある。
幸いにして、理工学に現れる多くの行列は $(3.39)$ の条件で見当をつけられる。

## 3.8 SOR 法

### 3.8.1 SOR 法

ガウス・ザイデル法 $(3.43)$ は、第 $k + 1$ 回目の $x_i^{(k+1)}$ は第 $k$ 回目の $x_i^{(k)}$ を $r_i^{(k)} / a_{ii}$ だけ補正して求める。
SOR(Successive Over-Relaxation — 逐次過緩和) 法はこの修正を少し大きめに行い、収束を加速しよういうもので、次の公式による。

$$
\begin{array}{l}
\text{SOR 法} \\
\quad x_i^{(k+1)} = x_i^{(k)} + \omega r_i^{(k)} / a_{ii} \qquad (1 \leqq i \leqq n) \\
\qquad ただし \quad r_i^{(k)} = b_i - \sum\limits_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \sum\limits_{j=i}^n a_{ij} x_j^{(k)}
\end{array} \tag{3.47}
$$

$\omega$ を緩和係数という。
加速というからには $\omega > 1$ でなければならないが、まれには $\omega < 1$ のこともある<sup>[$5)$](#note5)</sup> 。
$\omega = 1$ のときは、ガウス・ザイデル法に一致する。

> <small id="note5">$^{5)}$ このとき SUR(Successive Under-Relaxation) 法ということがある。</small>

### 3.8.2 SOR 法が収束するための必要条件

この手順を前節の $L$ 、 $D$ 、 $U$ で表わし、反復行列を求めると

$$
\begin{aligned}
x^{(k+1)} & = (D + \omega L)^{-1} [ (1 - \omega)D - \omega U ] x^{(k)} + \omega(D + \omega L)^{-1} b \\
& = H_\text{SOR} x^{(k)} + c_\text{SOR}
\end{aligned} \tag{3.48}
$$

ここに、

$$
\begin{align}
H_\text{SOR} & = (D + \omega L)^{-1} [ (1 - \omega)D - \omega U ] \qquad (反復行列) \tag{3.49} \\
c_\text{SOR} & = \omega(D + \omega L)^{-1}b \\
\end{align}
$$

SOR 法が収束するための必要条件を求める。
まず

$$
\vert \det H_\text{SOR} \vert = \vert \lambda_1 \lambda_2 \cdots \lambda_n \vert \leqq \rho_\text{SOR}^n
$$

であるであることに注意する。
ここに $\rho_\text{SOR}$ は $H_\text{SOR}$ のスペクトル半径である。
一方

$$
\begin{aligned}
\det H_\text{SOR} & = (\det (D + \omega_L))^{-1} \cdot \det [ (1 - \omega)D - \omega U ] \\
& = (\det D)^{-1} \cdot \det [ (1 - \omega)D ] \\
& = (\det D)^{-1} \cdot \det D \cdot (1 - \omega)^n \\
& = (1 - \omega)^n \\
\end{aligned}
$$

故に、緩和係数 $\omega$ と $\rho_\text{SOR}$ の間には

$$
\vert 1 - \omega \vert \leqq \rho_\text{SOR} \tag{3.50}
$$

の関係がある。
SOR 法の収束の必要十分条件は、言うまでもなく $ \rho_\text{SOR} < 1$ である。

故に、収束する $( \rho_\text{SOR} < 1)$ ための必要条件は、

$$
\vert 1 - \omega \vert < 1 すなわち 0 < \omega < 2 \tag{3.51}
$$

である。
ただし、これは十分条件ではない。
$\vert 1 - \omega \vert < 1$ であっても収束する $( \vert 1 - \omega \vert \leqq \rho_\text{SOR} < 1)$ こともあるし、収束しない $( \vert 1 - \omega \vert < 1 \leqq \rho_\text{SOR})$ こともある。
(例えば、 $\omega = 1$ のガウス・ザイデル法は収束することもあるし、収束しないこともある。) 言い方を変えれば、 $1 < \vert 1 - \omega \vert ( \leqq \rho_\text{SOR})$ ならば、収束することはない。

### 3.8.3 最適緩和係数

SOR 法の反復回数は $\omega$ の値に強く依存する。
収束を速くするには $\omega$ を適当に選んで、 $\rho_\text{SOR}$ を小さくすればよい。
$\rho_\text{SOR}$ を最も小さくするような $\omega$ を最適緩和係数と言い $\omega_\text{opt}$ と書けば、 $\omega_\text{opt}$ とその時の $\rho_\text{SOR}$ は

$$
\omega_\text{opt} = \frac{2}
{1 + \sqrt{1 - \rho_\text{J}^2}}, \qquad \rho_\text{SOR} = \omega_\text{opt} - 1 \tag{3.52}
$$

であることが知られている (ヤング (Young) の公式<sup>[$6)$](#note6)</sup> )。
ここに $\rho_\text{J}$ はヤコビ法の反復行列のスペクトル半径である。
$\rho_\text{J}$ のある程度の推定は $(3.36)$ で示したように行なわれる。
傾向として、行列の次数 $n$ が小さいときは $$\omega_\text{opt}$ は $1$ に近く、大きいときには $2$ に近い。
すなわち、 $n$ が小さいときはガウス・ザイデル法はよい方法である。
$n$ が大きい時は、反復回数は緩和係数に敏感に依存する。

> <small id="note6">$^{6)}$ R.S. バ－ガ：計算機による大型行列の反復解法, サイエンス社, 1972年</small>

> #### ［例 3.3］
>
> $n = 2$ の係数行列が $\begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}$ のときは、 $min{0.5, 0.5} \leqq \rho_\text{J} \leqq max{0.5, 0.5}$ より $\rho_\text{J} = 0.5$ 、したがって、 $\omega_\text{opt} = 1.0718 \cong 1、\rho_\text{SOR} = 0.0718$ である。
> また、このときのガウス・ザイデル法の反復行列 $H_\text{GS} = -(D + L)^{-1} U = \begin{pmatrix} 0 & -0.5 \\ 0 & 0.25 \end{pmatrix}$ のスペクトル半径 $\rho_\text{GS}$ は $\rho_\text{GS} = 0.25$ 。
> 故に、 $\rho_\text{SOR} \cong \rho_\text{GS}^{1.9} \cong \rho_\text{J}^{3.8}$ 。
> すなわち、SOR 法の反復1回はガウス・ザイデル法のほぼ2回、ヤコビ法のほぼ4回に相当する。
> このように、最適緩和係数 $\omega_\text{opt}$ がわかっているときには、SOR 法は大変有利である。

> #### ［例 3.4］
>
> 一般に、 $n$ 次係数行列 $A$ の要素が $a_{ii} = 2$ 、$a_{ii+1} = a_{ii-1} = \pm1$ 、その他の $a_{ij} = 0$ である3重対角実対称行列の場合のヤコビ法の反復行列 $H_\text{J} = -D^{-1} (L+U)$ のスペクトル半径 $\rho_\text{J}$ (問題 4-15 参照。 $n = 2$ のときは [例 3.3]) は、
>
> $$
> \rho_\text{J} = \max_{1 \leqq j \leqq n} \left\{ \left\vert \cos \left( \frac{j \pi}{n + 1} \right) \right\vert \right\} = \cos \left( \frac{\pi}{n + 1} \right)
> $$
>
> したがって、 $n$ が大きくなると、 $\rho_\text{J} \to 1$ 、 $\omega_\text{opt} \to 2$ に近づく。
> 例えば
>
> $$
> \begin{array}{c}
>     n = 10 \text{では} & \rho_\text{J} = 0.9595, & \omega_\text{opt} = 1.5604, & \rho_\text{SOR} = 0.5604 \cong \rho_\text{J}^{14} \\
>     n = 20 \text{では} & \rho_\text{J} = 0.9888, & \omega_\text{opt} = 1.7406, & \rho_\text{SOR} = 0.7406 \cong \rho_\text{J}^{27} \\
>     n = 30 \text{では} & \rho_\text{J} = 0.9949, & \omega_\text{opt} = 1.8163, & \rho_\text{SOR} = 0.8163 \cong \rho_\text{J}^{39} \\
> \end{array}
> $$
>
> であるから、SOR 法の反復1回は、 $n = 10$ ではヤコビ法の約14回、 $n = 20$ では約27回、$n = 30$ では約39回に相当する。

### 3.8.4 SOR 法の手順

<details><summary></summary><div>

図 3.6 に SOR 法の手順を2つの PAD で示す。
一般的に言って、行列が大きいときは最適緩和係数 $\omega_\text{opt}$ がくわしくわかっていない限り、SOR 法は使用すべきでな
い。
参考のために2番目の PAD に、緩和係数がわかっていないときに、計算を進めながら、緩和係数を推定して使用する手順 (バ－ガ (Varga) の方法) を 図 3.6 に併せて示す。
</div></details>

## 3.9 共役勾配法

### 3.9.1 関数の最小値探索

連立1次方程式の数値解法の1つに共役勾配法 (conjugate gradient method、略して CG 法) と呼ばれている方法がある。
この方法は、 $A$ を対称行列とするとき、連立1次方程式

$$
A x = b \qquad \text{(} A \text{は対称行列)} \tag{3.53}
$$

の解 $x$ は、 $n$ 次元ベクトル $x$ の関数

$$
F (x) = \frac{1}{2} (x, Ax) - (b, x) \tag{3.54}
$$

を最小にする $x$ と同じであることを利用している。
まず、このことを示そう。
ここに、 $(∗, ∗)$ はベクトルのスカラ－積を表し、

$$
(u, v) \equiv \sum_i u_i v_i = u^\top v = v^\top u = (v, u)
$$

とも書ける。
$F(x)$ は成分で書けば、

$$
F (x) = \frac{1}{2} \sum_{i,j} a_{ij} x_i x_j - \sum_i b_i x_i
$$

である。
$F(x)$ を $x_i$ で微分して $0$ と置くと、 $(3.53)$ が得られる：

$$
- \frac{\partial F(x)}{\partial x_i} = b_i - \sum_j a_{ij} x_j = 0 \qquad (1 \leqq i \leqq n) \tag{3.55}
$$

すなわち、 $F(x)$ が停留値 (極値) をもてば、その停留値を与える $x$ は $(3.53)$ の解であり、したがって、 $(3.53)$ を解くかわりに、 $F(x)$ が停留値をとる $x$ を求めてもよい。
共役勾配法は、関数 $F(x)$ が停留値、とくに最小値をとるような $x$ を効率よく求める巧妙な方法である。

しかし、 $(3.54)$ の関数 $F(x)$ は $x$ の2次式であるから、いつも最小値を持つとは限らず、また最小値を持っても、 $F(x)$ の最小値を与える $x$ はただ一つとは限らない。
そこで、まず、 $F(x)$ がただ一つの最小値を与える点 $x$ を持つためには、行列 $A$ が対称正定値でなければならないことを示す。
その後に、関数の最小値をあたえる $x$ の探索法を考えることにする。
